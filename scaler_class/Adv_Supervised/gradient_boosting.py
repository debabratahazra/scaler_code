import numpy as np
#import decision trees for regression
from sklearn.tree import DecisionTreeRegressor

x = np.asarray([[1.1378268656304509, -0.1691572511419156], [-0.4597805236957294, -1.0890344352371168], [0.9841172931027632, -1.1592063247728384], [-1.3802108750499156, -0.2616720669923489], [-0.1793796928832663, -0.692770576575069], [-1.4005412673495643, -0.18435057664102994], [-0.25766375938677644, 0.0503707161894736], [0.1762387523161263, -0.9403535422153736], [-0.7639136722275016, -0.4980731026078258], [-1.3911931153065715, 0.03625974169359078], [-0.6296419090890126, -0.4722584041747191], [-0.8864062679671508, 1.9213469553611742], [-1.7848038867517189, -0.7961858391647482], [1.0475853374178141, 0.22858201102290415], [-1.5133610372500914, 1.107624678074931], [0.9295943283931953, -1.0627949207805292], [0.7133895705195379, -0.7280577188284076], [0.8395164580409145, 1.2390209790064357], [-0.43653709264274676, 1.0092445309145563], [-0.3628911138637164, 0.26396030895579214]]   )
y = np.asarray([25.041416112606452, -76.82184874434061, -36.55168315784781, -57.38581902735705, -45.52881100199741, -53.65852172990456, -4.97308803080433, -48.95869988564721, -51.583197706557876, -40.36545428374305, -46.8239559217708, 83.78983643966608, -100.57682333446509, 45.17968792285051, 17.72700409636257, -33.05520934509737, -19.812339926586027, 97.25161995589107, 44.871827591938384, 4.282726474615903])
lr = 0.1
m = 20  # number of trees

def fit(x, y, m, learning_rate):
    """
    x : attributes
    y : target variable
    m : number of decision trees
    learning_rate : learning rate
    """

    # list consisting of weak Decision trees regressors
    weak_learners = []
    
    # Initialize the predictions with stage 0 predictions (mean)
    predictions = [np.mean(y)]*len(y)
    
    # Iterating over the number of estimators
    for _ in range(0, m):
    
        # Calculating the residuals
        residuals = [y[i] - predictions[i] for i in range(len(y))]
    
        # Creating a weak learner 
        weak_learner = DecisionTreeRegressor(max_depth = 1)

        # Training the tree on the residuals
        weak_learner.fit(x, residuals)

        # Appending the weak learner to the list
        weak_learners.append(weak_learner)

        # Getting the weak learner predictions
        predictions_wl = [weak_learner.predict(i.reshape(1,-1)) for i in x] 

        # Updating the current predictions
        predictions = [predictions[i] + learning_rate*predictions_wl[i] for i in range(len(x))]
    
    return predictions

print(fit(x, y, m, lr))


######################################
x = [[-0.5278716792389474, 0.32312129151895763], [0.016524293977752028, -1.2259512292676606], [-0.8124856763296616, -0.42673267964070544], [0.2610129059654795, 1.4536885687047405], [-1.5814548685473164, 0.7006671541880247], [0.9039572493061059, 1.7668052270854673], [0.7666831331487464, -0.16272352059749226], [-2.002349223944011, 0.2605323613477487], [2.637857773683349, -0.932423147852344], [0.6546731552013735, -0.35111903973965686], [-0.4426027341719403, 2.86052787285317], [-0.7757747251631579, 0.9328980003734091], [0.8671751092789988, 0.9871049738582621], [-0.04880612656096476, -2.2071525911498404], [-0.2500552647202252, 0.19065726274361913], [0.7012683173480152, -0.05226238924635941], [-1.4768989582736625, -0.33718273482602606], [0.9825595433980231, 0.19041562399360867], [-0.28793133303378105, -0.902972052661113], [0.538064444088734, -0.5817840542085159], [0.06634340374024432, 0.1831917984248655], [0.46484299221135417, 0.4020731649511768], [-0.42035639858153406, 0.1689768813733633], [-1.0010128257006834, -1.6616191023880666], [0.6924737471378254, -1.2876096107757498], [1.9914862177583634, -1.4613208497799748], [-2.40108771850749, -0.8575469992693593], [-0.4577439988968104, -1.3204434093783424], [0.7889508412122753, -2.5398600262131383], [-1.0980905531106537, 1.1665475115821418], [0.3381676367416322, 0.4898282154469753], [1.7948972337761646, 0.3913755802659664], [1.472225716392956, 0.1780256784852325], [-0.12027270618220942, 0.930375798137404], [-1.3004569350457058, -0.0982352288248432], [-0.259176551910237, 0.31394611469868733], [-1.515274084396903, -0.21638395977626068], [-0.767116780435318, 0.26263713096655084], [2.017560839251388, -0.15445201039115894], [0.5177239552639497, 0.4329580097638568], [1.67352732506769, 1.4907787938348371], [-0.20451922129339192, 0.39027761763767455], [0.5870295309381075, -1.3509821755929772], [-2.241976487586482, -0.7438630376253262], [-0.06465248303900237, -0.446174959631238], [-0.6494962619332793, 0.1208998448619223], [1.1264157163192527, 0.08274350379275111], [-0.1268980375340618, -0.5804442236853343], [1.2458012964193883, 2.9718521575646113], [-0.4401280863518335, -0.9874268121643319], [-0.6217012778637273, -0.1292471608584316], [-0.10876792449945656, -1.918231394884487], [1.1460905973170965, -1.8140407706950705], [0.08583324703044545, -1.7485485903234819], [0.9016648293702483, 0.06932711521350979], [-1.0306023890340537, -0.603679936059295], [0.6284074013714586, -0.03446980845000246], [0.04169111031784691, 1.2873241914568097], [-0.6400630654268803, -2.267825022100728], [-1.346310746676619, 0.40725601266115424]]
y = [-39.06807750217675, -12.697515059995046, -70.77224334961818, 37.92692122960891, -120.10424456936062, 93.58712907085417, 60.056990909553505, -159.01481506301494, 202.5749894973674, 48.64005435393924, -2.604315148517569, -52.292545031654434, 81.64949844659309, -29.360185121007955, -18.066244107301966, 56.03530666273063, -123.58769584039258, 81.75302279534324, -33.833073079280574, 36.55430533634641, 7.558467952427655, 41.77502196630419, -32.11030777071307, -100.31384763421617, 41.17039702714495, 144.17585235022028, -204.4612965452768, -52.666642282577556, 34.353182849567546, -75.17555184020816, 32.86005506501643, 150.042638271976, 121.29082481661523, 0.6861175901185004, -106.34862387641697, -17.461509899269238, -125.20801473419479, -59.07799577019282, 161.74580260162114, 47.070393053966605, 153.06385329581252, -12.326918215899177, 31.737581035182856, -190.36042384072707, -9.901277508658957, -51.33034274402085, 92.04251869800191, -17.170292036014573, 135.46486159039273, -47.26172902014995, -51.768779893947745, -30.757232872340886, 71.73132535732738, -13.433612292867767, 74.01099866075629, -90.41893149608418, 50.504029606853074, 17.810587432260874, -78.27792997359813, -104.56163144703797]
lr = 0.05
def gbdt_predict(weak_learners, learning_rate, X_test, y_mean):
    """
    Given the list of weak_learners and learning rate use the trained decision trees to predict the response variable for the observations in x
    """
    
    if learning_rate <= 0:
        raise ValueError("Learning rate must be positive")
    elif learning_rate > 1:
        raise ValueError("Learning rate must be less than or equal to 1")
    
    if weak_learners is None or len(weak_learners) == 0:
        raise ValueError("Weak learners list cannot be empty")
    
    if X_test is None or len(X_test) == 0:
        raise ValueError("Test data cannot be empty")
    
    #initialize the output with stage 0 prediction
    yhat = np.full(X_test.shape[0], y_mean)
    
    # Calculate the output for the observations in the x according to gradient boosting's all models
    for i in range(len(weak_learners)):
        yhat += learning_rate * weak_learners[i].predict(X_test)
        
    return yhat